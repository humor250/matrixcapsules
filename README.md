## ç”¨EMè·¯ç”±å®ç°çš„çŸ©é˜µèƒ¶å›Šï¼ˆä¸­æ–‡è¯‘æœ¬ï¼‰
        
Geoffrey Hinton, Sara Sabour, Nicholas Frosst{geoffhinton, sasabour, frosst}@google.com
è°·æ­Œå¤§è„‘   å¤šä¼¦å¤š, åŠ æ‹¿å¤§

### æ‘˜è¦
ä¸€ä¸ªèƒ¶å›Šæ˜¯ä¸€ç»„ç¥ç»å…ƒï¼Œå…¶è¾“å‡ºè¡¨å¾åŒä¸€å®ä½“çš„ä¸åŒå±æ€§ã€‚ä¸€ä¸ªèƒ¶å›Šç½‘ç»œçš„æ¯å±‚å«æœ‰å¤šä¸ªèƒ¶å›Šã€‚æˆ‘ä»¬æè¿°ä¸€ç§èƒ¶å›Šç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯ä¸ªèƒ¶å›Šæœ‰ä¸€ä¸ªé€»è¾‘å•å…ƒæ¥è¡¨æ˜ä¸€ä¸ªå®ä½“çš„å­˜åœ¨æ€§å’Œä¸€ä¸ª4x4çŸ©é˜µï¼Œè¿™ä¸ªçŸ©é˜µèƒ½å¤Ÿä¹ å¾—è¡¨å¾é‚£ä¸ªå®ä½“ä¸è§‚å¯Ÿè€…ï¼ˆå§¿æ€ï¼‰çš„å…³ç³»ã€‚æ¯å±‚çš„ä¸€ä¸ªèƒ¶å›Šå¯¹ä¸Šå±‚çš„å¤šä¸ªä¸åŒèƒ¶å›Šæ„æˆçš„å§¿æ€çŸ©é˜µè¿›è¡ŒæŠ•ç¥¨ï¼Œæ–¹æ³•æ˜¯å®ƒçš„å§¿æ€çŸ©é˜µä¸å¯è®­ç»ƒçš„è§†ç‚¹ä¸å˜çš„èƒ½å¤Ÿä¹ å¾—è¡¨å¾å±€éƒ¨-æ•´ä½“å…³ç³»çš„å˜æ¢çŸ©é˜µç›¸ä¹˜ã€‚æ¯å¼ é€‰ç¥¨é€šè¿‡ä¸€ä¸ªåˆ†é…çš„ç³»æ•°è¿›è¡ŒåŠ æƒã€‚æ¯å¼ å›¾ç‰‡é‡‡ç”¨Expectation-Maximization algorithmå¯¹è¿™äº›ç³»æ•°è¿›è¡Œè¿­ä»£æ›´æ–°ï¼Œè¿™æ ·ï¼Œæ¯ä¸ªèƒ¶å›Šçš„è¾“å‡ºè·¯ç”±åˆ°æ¥å—ä¸€ç»„ç›¸ä¼¼é€‰ç¥¨çš„ä¸Šå±‚çš„ä¸€ä¸ªèƒ¶å›Šã€‚å˜æ¢çŸ©é˜µçš„è®­ç»ƒä¸åŒï¼Œæ˜¯åœ¨æ¯å¯¹ç›¸é‚»èƒ¶å›Šå±‚ä¹‹é—´é‡‡ç”¨å±•å¼€å¼è¿­ä»£çš„EMç®—æ³•ï¼ˆunrolled iterations of EMï¼‰è¿›è¡Œåå‘ä¼ æ’­ã€‚é€šè¿‡smallNORBè¯„æµ‹, ä¸æœ€å¥½è®°å½•ç›¸æ¯”ï¼Œèƒ¶å›Šå‡å°‘äº†45%çš„æµ‹è¯•é”™è¯¯ç‡ã€‚åŒæ—¶æ˜¾ç¤ºå‡ºæ¯”æ ‡å‡†çš„CNNå¯¹ç™½ç›’å¯¹æŠ—æ”»å‡»å…·æœ‰è¶…å¼ºçš„æŠµæŠ—åŠ›ã€‚

------
### 1 ç®€ä»‹
å·ç§¯ç¥ç»ç½‘ç»œæ˜¯æ„å»ºäºè¿™æ ·çš„ç®€å•äº‹å®ï¼Œå³ä¸€ä¸ªè§†è§‰ç³»ç»Ÿè¦å¯¹å›¾ç‰‡ä¸­çš„æ‰€æœ‰ä½ç½®é‡‡ç”¨ç›¸åŒçš„çŸ¥è¯†ã€‚è¿™æ˜¯é€šè¿‡ç»‘å®šç‰¹å¾æ¢æµ‹å™¨çš„æƒé‡ï¼Œä»¥ä¾¿åœ¨ä¸€å¤„ä¹ å¾—çš„ç‰¹å¾åœ¨åˆ«å¤„æœ‰æ•ˆã€‚å·ç§¯èƒ¶å›Šç½‘ç»œæ‰©å±•ä½ç½®çŸ¥è¯†å…±äº«åˆ°åŒ…æ‹¬å±€éƒ¨-æ•´ä½“å…³ç³»çš„çŸ¥è¯†ï¼Œè¿™ç§å…³ç³»é€šè¿‡ä¸€ä¸ªç†Ÿæ‚‰çš„å›¾å½¢è¡¨ç¤ºã€‚è§†ç‚¹å˜åŒ–å¯¹åƒç´ å¼ºåº¦æœ‰å¤æ‚æ•ˆæœï¼Œä½†å¯¹è¡¨å¾å¯¹è±¡æˆ–å¯¹è±¡å±€éƒ¨å’Œè§‚å¯Ÿè€…ä¹‹é—´çš„å…³ç³»çš„å§¿æ€çŸ©é˜µæœ‰ç®€å•çº¿æ€§æ•ˆæœã€‚èƒ¶å›Šç½‘ç»œæ„å›¾åˆ©ç”¨å¥½è¿™ä¸€åº•å±‚çº¿æ€§å…³ç³»ï¼Œå¤„ç†è§†ç‚¹å˜åŒ–å’Œæå‡åˆ†å‰²å†³å®šåŠ›ã€‚èƒ¶å›Šç½‘ç»œåˆ©ç”¨é«˜ç»´åº¦å·§åˆè¿‡æ»¤ï¼šé€šè¿‡å¯»æ‰¾ç»™å§¿æ€çŸ©é˜µæŠ•ç¥¨çš„åè®®ï¼Œæ¢æµ‹å‡ºä¸€ä¸ªç†Ÿæ‚‰å¯¹è±¡ã€‚è¿™äº›ç¥¨æ¥è‡ªäºå·²ç»æ¢æµ‹å‡ºçš„å¯¹è±¡å±€éƒ¨ã€‚ä¸€ä¸ªå¯¹è±¡å±€éƒ¨äº§ç”Ÿä¸€ç¥¨ï¼Œæ–¹æ³•æ˜¯å®ƒçš„å§¿æ€çŸ©é˜µä¹˜ä»¥ä¸€ä¸ªä¹ å¾—çš„å˜æ¢çŸ©é˜µï¼Œå…¶è¡¨å¾è§†ç‚¹ä¸å˜çš„å±€éƒ¨å’Œæ•´ä½“å…³ç³»ã€‚éšç€è§†ç‚¹å˜åŒ–ï¼Œå±€éƒ¨å’Œæ•´ä½“çš„å§¿æ€çŸ©é˜µä¼šä»¥ä¸€ç§åè°ƒæ–¹å¼æ”¹å˜ï¼Œè¿™æ ·ï¼Œæ¥è‡ªä¸åŒå±€éƒ¨çš„æŠ•ç¥¨é—´çš„ä»»ä½•åè®®éƒ½ä¼šä¿æŒã€‚åœ¨ä¸€å †ä¸ç›¸å…³é€‰ç¥¨ä¸­å¯»æ‰¾é«˜ç»´é€‰ç¥¨çš„ç´§è‡´é›†ç¾¤æ˜¯ä¸€ç§è§£å†³å±€éƒ¨æ•´ä½“å½’å±é—®é¢˜çš„åŠæ³•ã€‚è¿™æ˜¯ä¸åŒå¯»å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¸èƒ½ç”¨å¯¹ä½ç»´åº¦ç¿»è¯‘ç©ºé—´ç½‘æ ¼åŒ–ä»¥åˆ©å·ç§¯é‚£æ ·ï¼Œå¯¹é«˜ç»´åº¦ä½“æ€ç©ºé—´ç½‘æ ¼åŒ–ã€‚å¯¹äºè¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨ç§°ä¸ºåè®®è·¯ç”±â€œroutingby-agreementâ€çš„å¿«é€Ÿè¿­ä»£å¤„ç†æ–¹æ³•ï¼Œå³å¯¹ä¸€ä¸ªå±€éƒ¨å±äºä¸€ä¸ªæ•´ä½“çš„æ¦‚ç‡è¿›è¡Œæ›´æ–°ï¼Œè¿™æ˜¯åŸºäºæ¥è‡ªäºé‚£ä¸ªå±€éƒ¨çš„é€‰ç¥¨æ¥è¿‘äºæ¥è‡ªå±äºé‚£ä¸ªæ•´ä½“çš„å…¶å®ƒå±€éƒ¨çš„é€‰ç¥¨ã€‚è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åˆ†å‰²åŸåˆ™ï¼Œå…¶å…è®¸é‡‡ç”¨ç†Ÿæ‚‰çš„å›¾å½¢çŸ¥è¯†æ´¾ç”Ÿåˆ†å‰²ï¼Œè€Œä¸æ˜¯ä»…ä»…ä½¿ç”¨å¦‚é¢œè‰²æˆ–é€Ÿåº¦çš„è¿‘ä¼¼å€¼æˆ–ä¸€è‡´æ€§ç­‰ä½çº§æ–¹æ³•ã€‚èƒ¶å›Šç½‘ç»œå’Œæ ‡å‡†ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªé‡è¦åŒºåˆ«åœ¨äºï¼Œä¸€ä¸ªèƒ¶å›Šçš„æ¿€æ´»æ˜¯åŸºäºä¸€ç§åœ¨å¤šä¸ªè¾“å…¥ä½“æ€é¢„æµ‹ä¹‹é—´çš„æ¯”è¾ƒï¼Œè€Œæ ‡å‡†ç¥ç»ç½‘ç»œæ˜¯åŸºäºåœ¨ä¸€ä¸ªå•ä¸€è¾“å…¥æ´»åŠ¨å‘é‡å’Œä¸€ä¸ªä¹ å¾—çš„æƒé‡å‘é‡çš„æ¯”è¾ƒã€‚

### 2 èƒ¶å›Šç½‘ç»œå¦‚ä½•å·¥ä½œ
å…¸å‹åœ°ï¼Œç¥ç»ç½‘ç»œåº”ç”¨ç®€å•çš„éçº¿æ€§ï¼Œå³åº”ç”¨ä¸€ä¸ªéçº¿æ€§å‡½æ•°è¿›è¡Œä¸€ä¸ªçº¿æ€§è¿‡æ»¤å™¨çš„æ ‡é‡è¾“å‡ºã€‚ä¹Ÿå¯ä»¥ç”¨softmaxçš„éçº¿æ€§å°†ä¸€ä¸ªå…¨éƒ¨logitså‘é‡è½¬åŒ–æˆä¸€ä¸ªæ¦‚ç‡å‘é‡ã€‚
èƒ¶å›Šç½‘ç»œç”¨ä¸€ä¸ªå¤æ‚å¾—å¤šçš„éçº¿æ€§æ–¹æ³•å°†ä¸€å±‚ä¸­çš„å…¨éƒ¨æ¿€æ´»æ¦‚ç‡å’Œèƒ¶å›Šå§¿æ€é›†åˆè½¬åŒ–ä¸ºä¸‹ä¸€å±‚çš„æ¿€æ´»æ¦‚ç‡å’Œèƒ¶å›Šå§¿æ€ã€‚ä¸€ä¸ªèƒ¶å›Šç½‘ç»œåŒ…å«å‡ ä¸ªèƒ¶å›Šå±‚ã€‚åœ¨Lå±‚çš„è¿™ç»„èƒ¶å›Šç”¨$â„¦_L$è¡¨ç¤ºã€‚æ¯ä¸ªèƒ¶å›Šæœ‰ä¸€ä¸ª4x4å§¿æ€çŸ©é˜µï¼ŒMï¼Œå’Œä¸€ä¸ªæ¿€æ´»æ¦‚ç‡ï¼Œaã€‚å®ƒä»¬å°±åƒä¸€ä¸ªæ ‡å‡†ç¥ç»ç½‘ç»œçš„æ´»åŠ¨ï¼šä¾èµ–äºå½“å‰è¾“å…¥ï¼Œä¸è¢«ä¿å­˜ã€‚åœ¨Lå±‚çš„æ¯ä¸ªèƒ¶å›Šiå’ŒL+1å±‚çš„æ¯ä¸ªèƒ¶å›Šjä¹‹é—´æœ‰ä¸€ä¸ª4x4çš„å¯è®­ç»ƒçš„è½¬æ¢çŸ©é˜µï¼Œ$W_{ij}$ã€‚è¿™äº›$ W_{ij} $ï¼ˆå’Œæ¯ä¸ªèƒ¶å›Šä¸¤ä¸ªä¹ å¾—çš„åç½®é¡¹ï¼‰æ˜¯å”¯ä¸€ä¿å­˜å‚æ•°ï¼Œè€Œä¸”å®ƒä»¬æ˜¯åˆ†åˆ«ä¹ å¾—çš„ã€‚èƒ¶å›Šiçš„å§¿æ€çŸ©é˜µç”±$W_{ij}$è¿›è¡Œå˜æ¢ï¼Œå³å¯¹èƒ¶å›Šjçš„å§¿æ€çŸ©é˜µæŠ•ä¸€ç¥¨$V_{ij} = M_iW_{ij}$ã€‚ä»¥$V_{ij}$ å’Œ $a_iï¼ˆi âˆˆ â„¦_L, j âˆˆ â„¦_{L+1}ï¼‰$ä¸ºè¾“å…¥ï¼Œç”¨éçº¿æ€§è·¯ç”±ç¨‹åºå¯¹L+1å±‚çš„å…¨éƒ¨èƒ¶å›Šçš„å§¿æ€å’Œæ¿€æ´»è¿›è¡Œè®¡ç®—ã€‚è¿™ä¸ªéçº¿æ€§ç¨‹åºæ˜¯EMç¨‹åºçš„ä¸€ä¸ªç‰ˆæœ¬ã€‚å®ƒè¿­ä»£åœ°è°ƒæ•´L+1å±‚èƒ¶å›Šçš„å‡å€¼ï¼Œå˜åŒ–å’Œæ¿€æ´»æ¦‚ç‡ï¼Œä»¥åŠåœ¨æ‰€æœ‰$i âˆˆ â„¦_L, j âˆˆ â„¦_{L+1}$ä¹‹é—´çš„åˆ†é…æ¦‚ç‡ã€‚åœ¨é™„å½•1ï¼Œæˆ‘ä»¬ç»™åè®®è·¯ç”±ï¼ˆrouting-by-agreementï¼‰ä¸€ä¸ªä¸­æ€§çš„ç›´è§‚ä»‹ç»ï¼Œå¹¶è¯¦ç»†æè¿°å®ƒä¸æ‹Ÿåˆé«˜æ–¯æ··åˆçš„EMç®—æ³•ä¹‹é—´çš„å…³ç³»ã€‚

### 3 ç”¨EMå®ç°è·¯ç”±åè®®
å‡å®šï¼Œæˆ‘ä»¬å·²ç»ç¡®å®šäº†ä¸€å±‚ä¸­æ‰€æœ‰èƒ¶å›Šçš„å§¿æ€å’Œæ¿€æ´»æ¦‚ç‡ï¼Œç°åœ¨ï¼Œæˆ‘ä»¬æƒ³è¦ç¡®å®šä¸Šå±‚æœ‰å“ªäº›æ¿€æ´»èƒ¶å›Šï¼Ÿä»¥åŠå¦‚ä½•å°†æ¯ä¸ªæ¿€æ´»çš„ä½å±‚èƒ¶å›Šåˆ†é…ç»™ä¸€ä¸ªæ¿€æ´»çš„é«˜å±‚èƒ¶å›Šã€‚é«˜å±‚ä¸­çš„æ¯ä¸ªèƒ¶å›Šå¯¹åº”ä¸€ä¸ªé«˜æ–¯ï¼ˆGaussianï¼‰ï¼Œä½å±‚ï¼ˆè½¬æˆäº†ä¸€ä¸ªå‘é‡ï¼‰çš„æ¯ä¸ªæ¿€æ´»èƒ¶å›Šçš„å§¿æ€å¯¹åº”ä¸€ä¸ªæ•°æ®ç‚¹ï¼ˆæˆ–è€…æ•°æ®ç‚¹çš„ç‰‡æ–­ï¼Œèƒ¶å›Šéƒ¨åˆ†æ¿€æ´»çš„æƒ…å†µä¸‹ï¼‰åˆ©ç”¨æœ€çŸ­æè¿°é•¿åº¦åŸåˆ™ï¼Œå½“å†³å®šæ˜¯å¦æ¿€æ´»ä¸€ä¸ªé«˜å±‚èƒ¶å›Šæ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ç§é€‰æ‹©ã€‚

é€‰æ‹©0: å¦‚æœä¸æ¿€æ´»å®ƒï¼Œè¦æè¿°æ‰€æœ‰åˆ†é…ç»™é«˜å±‚èƒ¶å›Šçš„ä½å±‚èƒ¶å›Šçš„å§¿æ€ï¼Œæˆ‘ä»¬å¿…é¡»ä¸ºæ¯ä¸ªæ•°æ®ç‚¹ä»˜å‡º$âˆ’Î²u$çš„å›ºå®šå¼€é”€ã€‚åœ¨éé€‚å½“å‡åŒ€å…ˆéªŒåˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªå¼€é”€æ˜¯æ•°æ®ç‚¹çš„è´Ÿå¯¹æ•°æ¦‚ç‡å¯†åº¦ã€‚å¯¹äºç‰‡æ®µåˆ†é…ï¼Œæˆ‘ä»¬ä»˜å‡ºå›ºå®šå¼€é”€çš„ç‰‡æ®µã€‚

é€‰æ‹©1: å¦‚æœæ¿€æ´»æ›´é«˜çº§åˆ«çš„èƒ¶å›Šï¼Œæˆ‘ä»¬å¿…é¡»ä»˜å‡º$âˆ’Î²a$çš„å›ºå®šå¼€é”€æ¥ç¼–ç å®ƒçš„å‡å€¼å’Œæ–¹å·®ï¼Œä»¥åŠå®ƒæ˜¯æ¿€æ´»çš„äº‹å®ï¼Œç„¶åæ”¯ä»˜é¢å¤–çš„è´¹ç”¨ï¼Œå¹¶æŒ‰ç…§åˆ†é…æ¦‚ç‡è¿›è¡Œæ¯”ä¾‹åˆ†é…ï¼Œä»¥æè¿°è¾ƒä½çº§åˆ«çš„å‡å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™æ˜¯å½“æ›´é«˜çº§åˆ«èƒ¶å›Šçš„å‡å€¼é€šè¿‡è½¬æ¢çŸ©é˜µçš„é€†å‘æ–¹æ³•æ¥é¢„æµ‹å®ƒä»¬æ—¶éœ€è¦çš„ã€‚è®¡ç®—æè¿°ä¸€ä¸ªæ•°æ®ç‚¹æˆæœ¬çš„ç®€å•å¾—å¤šçš„æ–¹æ³•æ˜¯ï¼Œåœ¨æ— è®ºå½’å±å“ªä¸ªé«˜å±‚èƒ¶å›Šæ‹Ÿåˆçš„é«˜æ–¯åˆ†å¸ƒä¸‹ï¼Œä½¿ç”¨é‚£ä¸ªæ•°æ®ç‚¹çš„é€‰ç¥¨çš„è´Ÿå¯¹æ•°æ¦‚ç‡å¯†åº¦ã€‚

å¯¹äºé™„å½•1è§£é‡Šçš„ç†ç”±ï¼Œæ˜¯ä¸æ­£ç¡®çš„ï¼Œä¸è¿‡ï¼Œæˆ‘ä»¬ç”¨å®ƒæ˜¯å› ä¸ºå®ƒéœ€è¦æ›´å°‘çš„è®¡ç®—ï¼ˆåœ¨é™„å½•ä¸­ä¹Ÿæœ‰è¯´æ˜ï¼‰ã€‚
é€‰æ‹©0å’Œ1åœ¨å¼€é”€æ–¹é¢çš„åŒºåˆ«æ˜¯ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­é€šè¿‡é€»è¾‘å‡½æ•°ç¡®å®šæ›´é«˜çº§åˆ«èƒ¶å›Šçš„æ¿€æ´»æ¦‚ç‡ã€‚é™„å½•1è§£é‡Šäº†é€»è¾‘å‡½æ•°æ˜¯æ­£ç¡®é€‰æ‹©çš„åŸå› ã€‚
ä½¿ç”¨æˆ‘ä»¬é’ˆå¯¹ä¸Šè¿°é€‰æ‹©1çš„é«˜æ•ˆè¿‘ä¼¼å€¼ï¼Œé€šè¿‡ä½¿ç”¨æœ‰è½´å¯¹é½åæ–¹å·®çŸ©é˜µçš„æ´»åŠ¨èƒ¶å›Šjæ¥è§£é‡Šæ•´ä¸ªæ•°æ®ç‚¹iäº§ç”Ÿçš„å¢é‡æˆæœ¬ï¼Œç®€å•åœ°è¯´ï¼Œå°±æ˜¯è§£é‡ŠæŠ•ç¥¨$V_{ij}$çš„æ¯ä¸ªç»´åº¦hçš„å…¨ç»´å¼€é”€æ€»å’Œã€‚ç®€å•æè¿°ä¸º$âˆ’ln(P^h_{i|j}) $ å…¶ä¸­ $P^h_{i|j}$æ˜¯çŸ¢é‡åŒ–é€‰ç¥¨$V_{ij}$çš„$h^{th}$ç»„ä»¶çš„æ¦‚ç‡å¯†åº¦ï¼Œè¿™æ˜¯åœ¨jå¯¹ç»´åº¦hçš„é«˜æ–¯æ¨¡å‹ä¸­ï¼Œæœ‰æ–¹å·®$(Ïƒ^h_j)^2$å’Œå‡å€¼$Âµ^h_j$ï¼Œå…¶ä¸­$Âµ_j$æ˜¯jçš„å§¿æ€çŸ©é˜µ$M_j$çš„çŸ¢é‡åŒ–ç‰ˆæœ¬ã€‚

$P^h_{i|j} = \frac{1}{\sqrt{2Ï€(Ïƒ^h_j)^2}}exp\Bigl(âˆ’\frac{(V^h_{ij} âˆ’ Âµ^h_j)^2}{2(Ïƒ^h_j)^2}\Bigr),
ln(P^h_{i|j}) = \frac{(V^h_{ij} âˆ’ Âµ^h_j)^2}{2(Ïƒ^h_j)^2} âˆ’ ln(Ïƒ^h_j) âˆ’ ln(2Ï€)/2$

å¯¹jçš„ä¸€ä¸ªå•ä¸ªç»´åº¦hï¼Œè®¡ç®—å…¶å…¨éƒ¨ä½å±‚èƒ¶å›Šçš„æ€»å’Œï¼Œå¾—åˆ°ï¼š

$$cost{^h_j} = \sum_iâˆ’r_{ij} ln(P^h_{i|j})$$

$$=\frac{\sum_ir_{ij} (V^h_{ij} âˆ’ Âµ^h_j)^2}{2(Ïƒ^h_j)^2}+(ln(Ïƒ^h_j) + \frac{ln(2Ï€)}2)\sum_ir_{ij}           (1)$$
$$= \biggl(ln(Ïƒ^h_j) + \frac{1+ln(2Ï€)}2\biggr) \sum_ir_{ij} $$

å…¶ä¸­$\sum_ir_{ij}$ æ˜¯åˆ†é…ç»™jçš„æ•°æ®é‡ï¼Œ$V^h_{ij}$æ˜¯$V_{ij}$åœ¨ç»´åº¦hä¸Šçš„å€¼ã€‚å¼€å¯èƒ¶å›Šjæé«˜äº†åˆ†é…ç»™jçš„è¾ƒä½çº§åˆ«èƒ¶å›Šå‡å€¼çš„æè¿°é•¿åº¦ï¼Œä»æ¯ä¸ªè¾ƒä½çº§åˆ«èƒ¶å›Šçš„$-Î²u$åˆ°$-Î²a$åŠ ä¸Šæ‰€æœ‰ç»´åº¦çš„æˆæœ¬æ€»å’Œï¼Œæ‰€ä»¥æˆ‘ä»¬å®šä¹‰èƒ¶å›Šjçš„æ¿€æ´»åŠŸèƒ½ä¸ºï¼š

$$a_j = logistic\biggl(Î»\bigl(Î²a âˆ’ Î²u\sum_ir_{ij} âˆ’\sum_hcost^h_j\bigr)\biggr) (2)$$

å…¶ä¸­ï¼Œ$Î²a$å¯¹äºæ‰€æœ‰èƒ¶å›Šè€Œè¨€æ˜¯ä¸€æ ·çš„ï¼Œè€Œä¸”ï¼Œ$Î»$æ˜¯ä¸€ä¸ªé€†å‘æ¸©åº¦å‚æ•°. æˆ‘ä»¬é€šè¿‡å·®å¼‚æ–¹æ³•å­¦ä¹ $Î²a$å’Œ$Î²u$ï¼Œå¹¶å°†$Î»$è®¾å®šä¸ºè¶…å‚æ•°ã€‚è¦å®ŒæˆL + 1å±‚èƒ¶å›Šçš„å§¿æ€å‚æ•°å’Œæ¿€æ´»ï¼Œæˆ‘ä»¬åœ¨Lå±‚å·²ç»å®Œæˆå§¿æ€å‚æ•°å’Œæ¿€æ´»ä¹‹åè¿è¡Œå‡ è½®EMç®—æ³•è¿­ä»£ï¼ˆé€šå¸¸ä¸º3è½®ï¼‰ã€‚ç”±æ•´ä¸ªèƒ¶å›Šå±‚å®ç°çš„éçº¿æ€§ç®—æ³•æ˜¯ä¸€ç§ä½¿ç”¨EMç®—æ³•çš„é›†ç¾¤å‘ç°å½¢å¼ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°ä¹‹ä¸ºEMè·¯ç”±ã€‚

----------

ç¨‹åº1è·¯ç”±ç®—æ³•è¿”å›å±‚L+1ä¸­çš„èƒ¶å›Šçš„æ¿€æ´»å’Œå§¿åŠ¿ï¼Œåœ¨ç»™å‡ºèƒ¶å›Šåœ¨å±‚Lä¸­çš„æ¿€æ´»å’ŒæŠ•ç¥¨æƒ…å†µä¸‹ã€‚$V^h_{ij}$æ˜¯æ¥è‡ªä»Lå±‚çš„æ¿€æ´»$a_i$èƒ¶å›Šiåˆ°L+1å±‚çš„èƒ¶å›Šjçš„ç¬¬hç»´é€‰ç¥¨. $Î²aï¼ŒÎ²u$æ˜¯åŒºåˆ«ä¹ å¾—çš„ï¼Œå¹¶ä¸”é€†å‘æ¸©åº¦Î»åœ¨æ¯æ¬¡è¿­ä»£ä¸­æŒ‰å›ºå®šæ—¶é—´è¡¨æé«˜ã€‚

----------
1: **procedure** EM ROUTING$(a, V )$

2: $âˆ€i âˆˆ â„¦_L, j âˆˆ â„¦_L+1: R_{ij} â† 1/|â„¦_L+1|$

3: for $t$ iterations do

4: $âˆ€j âˆˆ â„¦_L+1$: M-STEP$(a, R, V , j)$

5: $âˆ€i âˆˆ â„¦_L$: E-STEP$(Âµ, Ïƒ, a, V , i)$

return $a, M$


1: **procedure** M-STEP$(a, R, V , j)$ . ã€‹for one higher-level capsule, j

2: $âˆ€i âˆˆ â„¦_L: R_{ij} â† R_{ij} âˆ— a_i$

3: $âˆ€h: Âµ^h_j â†\frac{\sum_i R_{ij}V^h_{ij}}{\sum_i R_{ij}}$

4: $âˆ€h: (Ïƒ^h_j)^2 â†\frac{\sum_i Rij (V^h_{ij}âˆ’Âµ^h_j)^2}{\sum_i Rij}$

5: $cost^h â†\bigl(Î²u + log(Ïƒ^h_j)\bigr)\sum_i R_{ij}$

6: $a_j â† logistic(Î»(Î²_a âˆ’\sum_hcost^h))$


1: **procedure** E-STEP(Âµ, Ïƒ, a, V , i) . ã€‹for one lower-level capsule, i

2: $âˆ€j âˆˆ â„¦_{L+1}: p_j â† \frac{1}{\sqrt{\prod^H_h2Ï€(Ïƒ^h_j)2}}exp\bigl(âˆ’\sum^H_h\frac{(V^h_{ij}âˆ’Âµ^h_j)2}{2(Ïƒ^h_j)2}\bigr)$

3: $âˆ€j âˆˆ â„¦_{L+1}: R_{ij} â† \frac{a_j p_j}{\sum_{kâˆˆâ„¦_{L+1}}a_kp_k}$


### 4 èƒ¶å›Šç½‘ç»œæ¶æ„
æ¨¡å‹æ€»çš„æ¶æ„å¦‚å›¾1æ‰€ç¤ºã€‚
æ¨¡å‹ç”±ä¸€ä¸ª5x5å¸¦32é€šé“(A=32)ï¼Œç”¨ReLUéçº¿æ€§å‡½æ•°ï¼Œæ­¥é•¿ä¸º2çš„å·ç§¯å±‚å¼€å§‹ã€‚æ‰€æœ‰å…¶å®ƒå±‚æ˜¯èƒ¶å›Šå±‚ï¼Œå§‹äºä¸»èƒ¶å›Šå±‚ã€‚
B=32ä¸»èƒ¶å›Šç±»å‹çš„æ¯ä¸ªèƒ¶å›Šçš„4x4å§¿æ€æ˜¯ä¸€ä¸ªä¹ å¾—çš„æ‰€æœ‰çš„ä½å±‚çš„åœ¨é‚£ä¸ªä¸­å¿ƒweiReLUçš„çº¿æ€§è½¬æ¢ã€‚

![æ¶æ„å›¾](https://github.com/humor250/matrixcapsules/blob/master/cape.png)

å›¾1ï¼šèƒ¶å›Šç½‘ç»œæ¶æ„æœ‰ä¸€ä¸ªReLUå·ç§¯å±‚ï¼Œåé¢è·Ÿä¸€ä¸ªä¸»å·ç§¯èƒ¶å›Šå±‚å’Œä¸¤ä¸ªå…¶å®ƒå·ç§¯èƒ¶å›Šå±‚ã€‚

ä¸»èƒ¶å›Šçš„æ¿€æ´»æ˜¯åˆ©ç”¨sigmoidå‡½æ•°å¤„ç†åŒç»„çš„ä½å±‚ReLUçš„æƒé‡æ€»å’Œäº§ç”Ÿã€‚ä¸»èƒ¶å›Šä¹‹åæ˜¯ä¸¤ä¸ª3x3å·ç§¯èƒ¶å›Šå±‚ï¼ˆK = 3ï¼‰ï¼Œæ¯ä¸ªéƒ½æœ‰32ä¸ªèƒ¶å›Šç±»å‹ï¼ˆC = D = 32ï¼‰ï¼Œæ­¥å¹…åˆ†åˆ«ä¸º2å’Œ1ã€‚æœ€åä¸€å±‚å·ç§¯èƒ¶å›Šè¿æ¥åˆ°æ¯ä¸ªè¾“å‡ºçº§éƒ½æœ‰ä¸€ä¸ªèƒ¶å›Šçš„æœ€ç»ˆèƒ¶å›Šå±‚ã€‚å°†æœ€åä¸€ä¸ªå·ç§¯èƒ¶å›Šå±‚è¿æ¥åˆ°æœ€åä¸€å±‚æ—¶ï¼Œæˆ‘ä»¬ä¸æƒ³ä¸¢å¼ƒè¿œç¦»æœ‰å…³å·ç§¯èƒ¶å›Šä½ç½®çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¹Ÿæƒ³åˆ©ç”¨æ‰€æœ‰åŒä¸€ç±»å‹çš„èƒ¶å›Šéƒ½åœ¨ä¸åŒä½ç½®æå–åŒä¸€ä¸ªå®ä½“çš„äº‹å®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ†äº«åŒä¸€èƒ¶å›Šç±»å‹çš„ä¸åŒä½ç½®çš„å˜æ¢çŸ©é˜µï¼Œç„¶åå°†æ¯ä¸ªèƒ¶å›Šçš„æ¥å—åŸŸä¸­å¿ƒçš„ç¼©æ”¾åæ ‡ï¼ˆè¡Œï¼Œåˆ—ï¼‰æ·»åŠ åˆ°å®ƒçš„æŠ•ç¥¨çŸ©é˜µå³ä¾§æ ä¸­çš„å¤´ä¸¤ä¸ªå…ƒç´ ã€‚æˆ‘ä»¬ç§°è¿™ç§æŠ€æœ¯ä¸ºåæ ‡åŠ æˆã€‚è¿™åº”è¯¥æœ‰åŠ©äºè¿™ä¸ªå…±äº«çš„æœ€ç»ˆè½¬æ¢äº§ç”Ÿä»·å€¼ï¼Œå› ä¸ºè¿™ä¸¤ä¸ªå…ƒç´ è¡¨ç¤ºäº†è¿™ä¸ªç›¸å¯¹äºèƒ¶å›Šæ¥å—åŸŸä¸­å¿ƒçš„å®ä½“çš„ç²¾ç¡®ä½ç½®ã€‚

è·¯ç”±ç¨‹åºåœ¨æ¯å¯¹ç›¸é‚»çš„èƒ¶å›Šå±‚ä¹‹é—´ä½¿ç”¨ã€‚å¯¹äºå·ç§¯èƒ¶å›Šï¼ŒL+1å±‚çš„æ¯ä¸ªèƒ¶å›Šåªå°†åé¦ˆå‘é€åˆ°Lå±‚ä¸­å…¶æ¥å—åŸŸå†…çš„èƒ¶å›Šã€‚å› æ­¤ï¼ŒLå±‚çš„ä¸€ä¸ªèƒ¶å›Šçš„æ¯ä¸ªå·ç§¯å®ä¾‹ä»¥æœ€å¤§æ ¸å°ºå¯¸Xæ¥æ”¶æ¥è‡ªL+1å±‚çš„æ¯ä¸ªèƒ¶å›Šç±»å‹çš„æ ¸å°ºå¯¸åé¦ˆã€‚è¶Šæ¥è¿‘å›¾åƒè¾¹ç•Œçš„å®ä¾‹æ¥æ”¶è¾ƒå°‘çš„åé¦ˆï¼Œå¦‚è§’è½å®ä¾‹æ¥æ”¶ä»…ä¸€ä¸ªæ¥è‡ªL+1å±‚çš„ä¸€ä¸ªåé¦ˆã€‚

### 4.1 SPREAD LOSSä¼ æ’­æŸå¤±
In order to make the training less sensitive to the initialization and hyper-parameters of the model,
we use â€œspread lossâ€ to directly maximize the gap between the activation of the target class (at) and
the activation of the other classes. If the activation of a wrong class, ai
, is closer than the margin,
m, to at then it is penalized by the squared distance to the margin:
$Li = (max(0, m âˆ’ (a_t âˆ’ a_i))^2, L =\sum_{i \neq t}L_i (3)$
By starting with a small margin of 0.2 and linearly increasing it during training to 0.9, we avoid
dead capsules in the earlier layers. Spread loss is equivalent to squared Hinge loss with m = 1.
Guermeur & Monfrini (2011) studies a variant of this loss in the context of multi class SVMs.

ä¸ºäº†é™ä½è®­ç»ƒå¯¹æ¨¡å‹çš„åˆå§‹å‚æ•°å’Œè¶…å‚æ•°æ•æ„Ÿåº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨â€œä¼ æ’­æŸå¤±â€æ¥ç›´æ¥æœ€å¤§åŒ–ç›®æ ‡ç±»ï¼ˆ$a_t$ï¼‰æ¿€æ´»å’Œå…¶ä»–ç±»æ¿€æ´»ä¹‹é—´çš„é—´è·ã€‚å¦‚æœé”™è¯¯ç±»åˆ«$a_i$çš„æ¿€æ´»æ¯”å¯¹$a_t$ä½™é‡mæ›´è¿‘ï¼Œé‚£ä¹ˆå®ƒçš„ç½šé¢æ˜¯è·ç¦»å¹³æ–¹ï¼š$$L_i = (max(0, m âˆ’ (a_t âˆ’ a_i))^2, L =\sum_{i \neq t}L_i (3)$$ ä»0.2çš„å°å¹…åº¦å¼€å§‹ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å…¶çº¿æ€§å¢åŠ åˆ°0.9ï¼Œæˆ‘ä»¬é¿å…äº†æ—©æœŸå±‚ä¸­çš„æ­»èƒ¶å›Šã€‚ä¼ æ’­æŸå¤±ç›¸å½“äºm = 1æ—¶çš„HingeæŸå¤±å€¼çš„å¹³æ–¹ã€‚Guermeurï¼†Monfriniï¼ˆ 2011ï¼‰ç ”ç©¶äº†åœ¨å¤šç±»SVMèƒŒæ™¯ä¸‹è¿™ç§æŸå¤±çš„ä¸€ä¸ªå˜ä½“ã€‚

5 å®éªŒ

smallNORBæ•°æ®é›†ï¼ˆLeCun et al.ï¼ˆ2004ï¼‰ï¼‰æœ‰5ç§ç©å…·çš„ç°åº¦ç«‹ä½“å›¾åƒï¼šé£æœºï¼Œæ±½è½¦ï¼Œå¡è½¦ï¼Œäººç±»å’ŒåŠ¨ç‰©ï¼Œæ¯ç§æœ‰10ä¸ªæ¶‚å“‘å…‰ç»¿è‰²çš„ç‰©ç†å®ä¾‹ã€‚æ¯ç§çš„5ä¸ªç‰©ç†å®ä¾‹ä¸ºè®­ç»ƒæ•°æ®ï¼Œå¦å¤–5ä¸ªä¸ºæµ‹è¯•æ•°æ®ã€‚æ¯ä¸ªç©å…·éƒ½æœ‰18ä¸ªä¸åŒçš„æ–¹ä½è§’ï¼ˆ0-340ï¼‰ï¼Œ9ä¸ªé«˜åº¦å’Œ6ç§å…‰ç…§æ¡ä»¶ï¼Œæ‰€ä»¥è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†å‡åŒ…å«24,300ä¸ª96x96å›¾åƒçš„ç«‹ä½“å¯¹ã€‚æˆ‘ä»¬é€‰æ‹©smallNORBä½œä¸ºå¼€å‘èƒ¶å›Šç³»ç»Ÿçš„åŸºå‡†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸“ä¸ºä¸€ç§çº¯ç²¹çš„å›¾å½¢è¯†åˆ«ä»»åŠ¡è€Œè¿›è¡Œçš„ç»†è‡´è®¾è®¡ï¼Œä¸å—ä¸Šä¸‹æ–‡å’Œé¢œè‰²å¹²æ‰°ï¼Œä½†å®ƒæ¯”MNISTæ›´æ¥è¿‘è‡ªç„¶å›¾åƒã€‚

è¡¨1ï¼šæˆ‘ä»¬çš„èƒ¶å›Šæ¶æ„çš„ä¸åŒç»„ä»¶å¯¹smallNORBçš„å½±å“ã€‚
![è¡¨ä¸€](https://github.com/humor250/matrixcapsules/blob/master/table1_matrixcapsules.png)

æˆ‘ä»¬å°†smallNORBç¼©å‡ä¸º48Ã—48åƒç´ ï¼Œæ¯å¹…å›¾åƒæ­£å¸¸åŒ–ä¸ºæœ‰é›¶å‡å€¼å’Œå•ä½å·®å¼‚ã€‚
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éšæœºè£å‰ªå‡º32Ã—32å°å›¾ç‰‡å¹¶æ·»åŠ éšæœºäº®åº¦å’Œä¸è£å‰ªçš„å›¾åƒå½¢æˆå¯¹æ¯”ã€‚
åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»å›¾åƒä¸­å¿ƒå‰ªä¸‹ä¸€ä¸ª32Ã—32çš„å°å›¾ç‰‡ï¼Œå¹¶åœ¨smallNORBä¸Šå®ç°1.8ï¼…çš„æµ‹è¯•é”™è¯¯ã€‚å¦‚æœæˆ‘ä»¬å¹³å‡åŒ–æµ‹è¯•æ—¶å¤šä¸ªè£å‰ªçš„ç§ç±»æ¿€æ´»ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†1.4ï¼…ã€‚åœ¨ä¸ä½¿ç”¨å…ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒsmallNORBä¸Šæœ€å¥½çš„æŠ¥å‘Šç»“æœæ˜¯2.56ï¼…ï¼ˆCiresfortç­‰ï¼ˆ2011ï¼‰ï¼‰ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œä»–ä»¬æ·»åŠ äº†ä¸¤ä¸ªé¢å¤–çš„ç«‹ä½“å›¾åƒå¯¹è¾“å…¥ï¼Œå›¾åƒæ˜¯é€šè¿‡ä¸­å¿ƒæ»¤æ³¢å™¨å’Œç¦»å¿ƒæ»¤æ³¢å™¨åˆ›å»ºçš„ã€‚ä»–ä»¬ä¹Ÿå¯¹å›¾åƒåº”ç”¨ä»¿å°„å¤±çœŸã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜å‡»è´¥äº†Sabourç­‰äººï¼ˆ2017ï¼‰åœ¨smallNORBä¸Šè¾¾åˆ°2.7ï¼…çš„èƒ¶å›Šç½‘ç»œå·¥ä½œã€‚æˆ‘ä»¬è¿˜åœ¨NORBä¸Šæµ‹è¯•äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦å¢åŠ èƒŒæ™¯çš„smallNORBçš„ä¸€ä¸ªæŠ–åŠ¨ç‰ˆæœ¬ï¼Œæˆ‘ä»¬å®ç°äº†2.6ï¼…çš„é”™è¯¯ç‡ï¼Œçœ‹é½äº†2.7ï¼…çš„æœ€å¥½è®°å½•ï¼ˆCiresan et al.ï¼ˆ2012ï¼‰ï¼‰ã€‚

ä½œä¸ºæˆ‘ä»¬å¯¹æ–°è§†è§’è¿›è¡Œæ€»ç»“çš„å®éªŒåŸºå‡†ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªCNNï¼Œå¸¦æœ‰ä¸¤ä¸ªåˆ†åˆ«å…·æœ‰32å’Œ64é€šé“çš„å·ç§¯å±‚ã€‚ä¸¤å±‚éƒ½æœ‰ä¸€ä¸ªå†…æ ¸å¤§å°
ä¸º5ï¼Œæ­¥å¹…ä¸º1ï¼Œå¸¦ä¸€ä¸ª2Ã—2æœ€å¤§åŒ–æ± ã€‚ç¬¬ä¸‰å±‚æ˜¯1024ä¸ªå•å…ƒçš„å¸¦ä¸¢å¤±ï¼ˆdropoutï¼‰çš„å…¨è¿æ¥å±‚ï¼Œå¹¶è¿æ¥åˆ°5è·¯softmaxè¾“å‡ºå±‚ã€‚æ‰€æœ‰éšè—çš„å•å…ƒä½¿ç”¨ReLU
éçº¿æ€§ç®—æ³•ã€‚å¯¹CNNåŸºå‡†ï¼Œæˆ‘ä»¬å‡†å¤‡äº†ä¸Šè¿°å¯¹èƒ¶å›Šç½‘ç»œç›¸åŒçš„å›¾ç‰‡ã€‚æˆ‘ä»¬çš„åŸºå‡†CNNæ˜¯å¹¿æ³›çš„è¶…å‚æœç´¢ï¼ˆè¿‡æ»¤å™¨å¤§å°ï¼Œé€šé“æ•°é‡å’Œå­¦ä¹ ç‡ï¼‰çš„ç»“æœã€‚
CNNåŸºå‡†åœ¨smallNORBä¸Šè¾¾åˆ°5.2ï¼…çš„æµ‹è¯•é”™è¯¯ç‡ï¼Œæœ‰4.2Må‚æ•°é‡ã€‚æˆ‘ä»¬æ¨æ–­Ciresfortç­‰äººï¼ˆ2011ï¼‰ç½‘ç»œæ‹¥æœ‰2.7Må‚æ•°ã€‚é€šè¿‡ä½¿ç”¨å°çŸ©é˜µä¹˜æ³•ï¼Œä¸åŸºå‡†CNNç›¸æ¯”ï¼Œæˆ‘ä»¬å°†å‚æ•°æ•°é‡å‡å°‘äº†15åˆ°310Kï¼ˆå’ŒCiresfortç­‰äººï¼ˆ2011ï¼‰çš„9å€å› å­ï¼‰ã€‚ ä¸€ä¸ªåªæœ‰68Kå¯è®­ç»ƒå‚æ•°çš„A=64ï¼ŒB=8ï¼ŒC=D=16çš„å°èƒ¶å›Šç½‘ç»œï¼Œè¾¾åˆ°äº†2.2ï¼…çš„æµ‹è¯•é”™è¯¯ç‡ï¼Œè¿™ä¹Ÿå‡»è´¥äº†ä¹‹å‰çš„æœ€ä¼˜çš„æµ‹è¯•é”™è¯¯ç‡ã€‚

Fig. 2 shows how EM routing adjusts the vote assignments and the capsule means to find the tight
clusters in the votes. 
The histograms show the distribution of vote distances to the mean (pose) of
each class capsule during routing iterations. 

At the first iteration, votes are distributed equally between
5 final layer capsules. Therefore, all capsules receive votes closer than 0.05 to their calculated
mean. In the second iteration, the assignment probability for agreeing votes increases. Therefore,
most of the votes are assigned to the detected clusters, the animal and human class in the middle
row, and the other capsules only receive scattered votes which are further than 0.05 from the calculated
mean. The zoomed-out version of Fig. 2 in the Appendix shows the full distribution of vote
distances at each routing iteration.
Instead of using our MDL-derived capsule activation term which computes a separate activation
probability per capsule, we could view the capsule activations like the mixing proportions in a
mixture of Gaussians and set them to be proportional to the sum of the assignment probabilities
of a capsule and to sum to 1 over all the capsules in a layer. 

This increases the test error rate on

å›¾2 æ˜¾ç¤ºäº†EMè·¯ç”±å¦‚ä½•è°ƒæ•´æŠ•ç¥¨åˆ†é…å’Œèƒ¶å›Šå‡å€¼ï¼Œä»¥æ‰¾å‡ºé€‰ç¥¨ä¸­çš„ç´§è‡´ç¾¤ã€‚
ç›´æ–¹å›¾æ˜¾ç¤ºï¼Œåœ¨è·¯ç”±è¿­ä»£æœŸé—´ï¼Œé€‰ç¥¨è·ç¦»æ¯ç±»èƒ¶å›Šå‡å€¼ï¼ˆå§¿æ€ï¼‰çš„åˆ†å¸ƒã€‚åœ¨ç¬¬ä¸€è½®è¿­ä»£ä¸­ï¼ŒæŠ•ç¥¨åœ¨5ä¸ªæœ€åå±‚èƒ¶å›Šä¹‹é—´å‡ç­‰åˆ†å¸ƒã€‚å› æ­¤ï¼Œæ‰€æœ‰èƒ¶å›Šæ¥å—åˆ°çš„é€‰ç¥¨æ¯”0.05æ›´æ¥è¿‘å®ƒä»¬ç®—å‡ºçš„å‡å€¼ã€‚åœ¨ç¬¬äºŒè½®è¿­ä»£ä¸­ï¼Œæ¬¢è¿æŠ•ç¥¨çš„åˆ†é…æ¦‚ç‡å¢åŠ ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°é€‰ç¥¨éƒ½è¢«åˆ†é…åˆ°æ£€æµ‹åˆ°çš„é›†ç¾¤ï¼Œä¸­é—´è¡Œçš„åŠ¨ç‰©å’Œäººç±»ï¼Œè€Œå…¶ä»–èƒ¶å›Šåªæ¥æ”¶åˆ°é›¶æ•£é€‰ç¥¨ï¼Œå› å…¶è·ç¦»è®¡ç®—çš„å‡å€¼è¿œç¦»0.05ã€‚é™„å½•ä¸­å›¾2çš„ç¼©å°ç‰ˆå‡ºç¤ºäº†ï¼Œåœ¨æ¯è½®è·¯ç”±è¿­ä»£ä¸­é€‰ç¥¨è·ç¦»çš„å®Œæ•´åˆ†é…ã€‚è€Œä¸æ˜¯ä½¿ç”¨æˆ‘ä»¬çš„MDLæ´¾ç”Ÿçš„èƒ¶å›Šæ¿€æ´»æœ¯è¯­æ¥è®¡ç®—æ¯ä¸ªèƒ¶å›Šçš„å•ç‹¬æ¿€æ´»æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿèƒ¶å›Šæ¿€æ´»ï¼Œå¦‚åœ¨ä¸€ä¸ªé«˜æ–¯æ··åˆä¸­çš„æ··åˆæ¯”ä¾‹ï¼Œå¹¶å°†å®ƒä»¬è®¾ç½®ä¸ºä¸ä¸€ä¸ªèƒ¶å›Šçš„åˆ†é…æ¦‚ç‡æ€»å’Œæˆæ¯”ä¾‹ï¼Œå¹¶ä¸”åœ¨ä¸€å±‚ä¸­çš„æ‰€æœ‰èƒ¶å›Šä¸Šæ€»è®¡ä¸º1ã€‚è¿™å¢åŠ äº†æµ‹è¯•é”™è¯¯ç‡

![å›¾2](https://github.com/humor250/matrixcapsules/blob/master/pic2_matrixcapsules.png)

å›¾2ï¼šæ¯è½®è¿­ä»£åé€‰ç¥¨è·ç¦»åˆ°5ä¸ªæœ€åèƒ¶å›Šçš„æ¯ä¸ªå‡å€¼çš„ç›´æ–¹å›¾ã€‚æ¯ä¸ªè·ç¦»ç‚¹ç”±å…¶åˆ†é…æ¦‚ç‡åŠ æƒã€‚æ‰€æœ‰ä¸‰ä¸ªå›¾åƒæ˜¯ä»smallNORBæµ‹è¯•é›†ä¸­é€‰å‡ºçš„ã€‚åœ¨å¡è½¦å’Œäººç±»ä¾‹å­ä¸­ï¼Œè·¯ç”±ç¨‹åºæ­£ç¡®åœ°è·¯ç”±äº†é€‰ç¥¨ã€‚é£æœºä¾‹å­æ˜¾ç¤ºäº†ä¸€ä¸ªç½•è§çš„æ¨¡å‹å¤±è´¥çš„æ¡ˆä¾‹ï¼Œåœ¨ç¬¬ä¸‰æ¬¡è·¯çº¿è¿­ä»£ä¸­é£æœºä¸æ±½è½¦æ··æ·†ã€‚ç›´æ–¹å›¾è¢«æ”¾å¤§ä»¥å¯è§†åŒ–åªæœ‰è·ç¦»å°äº0.05çš„é€‰ç¥¨ã€‚å›¾B.2æ˜¾äº†â€œäººç±»â€èƒ¶å›Šçš„å®Œæ•´ç›´æ–¹å›¾ï¼Œæ²¡æœ‰å‰ªè£xè½´æˆ–å›ºåŒ–yè½´çš„æ¯”ä¾‹ã€‚

è¡¨2ï¼šåœ¨ç†Ÿæ‚‰è§†è§’ä¸‹ä¸¤æ¨¡å‹è¯¯å·®ç‡ç›¸åŒæ—¶ï¼Œåœ¨æ–°è§†è§’ä¸‹ï¼ŒåŸºçº¿CNNå’Œèƒ¶å›Šæ¨¡å‹çš„smalNORBæµ‹è¯•é”™è¯¯ç‡çš„æ¯”è¾ƒ
![è¡¨2](https://github.com/humor250/matrixcapsules/blob/master/table2_matrixcapsules.png)

smallNORBé™è‡³4.5ï¼…ã€‚æ ‡ç­¾.1ç»Ÿè®¡äº†è·¯ç”±è¿­ä»£æ¬¡æ•°çš„å½±å“ï¼Œå³ç±»å‹æŸå¤±ï¼Œä»¥åŠä½¿ç”¨çŸ©é˜µè€Œä¸æ˜¯å‘é‡æ¥è¡¨ç¤ºå§¿æ€ã€‚ä¸å›¾1ç›¸åŒçš„èƒ¶å›Šæ¶æ„ï¼Œåœ¨MNISTä¸Šè¾¾åˆ°äº†0.44ï¼…çš„æµ‹è¯•é”™è¯¯ç‡ã€‚å¦‚æœåœ¨ç¬¬ä¸€éšå±‚çš„é€šé“æ•°é‡å¢åŠ åˆ°256ä¸ªï¼Œåœ¨Cifar10ä¸Šå®ç°äº†11.9ï¼…çš„æµ‹è¯•é”™è¯¯ç‡ï¼ˆKrizhevskyï¼†Hintonï¼ˆ2009ï¼‰ï¼‰ã€‚

5.1 GENERALIZATION TO NOVEL VIEWPOINTS
A more severe test of generalization is to use a limited range of viewpoints for training and to test on
a much wider range. We trained both our convolutional baseline and our capsule model on one-third
of the training data containing azimuths of (300, 320, 340, 0, 20, 40) and tested on the two-thirds of
the test data that contained azimuths from 60 to 280. In a separate experiment, we trained on the 3
smaller elevations and tested on the 6 larger elevations.
It is hard to decide if the capsules model is better at generalizing to novel viewpoints because it
achieves better test accuracy on all viewpoints. To eliminate this confounding factor, we stopped
training the capsule model when its performance matched the baseline CNN on the third of the
test set that used the training viewpoints. Then, we compared these matched models on the twothirds
of the test set with novel viewpoints. Results in Tab. 2 show that compared with the baseline
CNN capsules with matched performance on familiar viewpoints reduce the test error rate on novel
viewpoints by about 30% for both novel azimuths and novel elevations.

5.1æ–°è§†è§’æ¦‚è¿°
æ›´ä¸¥æ ¼çš„æ€»ä½“æµ‹è¯•ï¼Œæ˜¯ä½¿ç”¨æœ‰é™èŒƒå›´çš„è§†è§’è¿›è¡Œè®­ç»ƒï¼Œå’Œæµ‹è¯•èŒƒå›´æ›´å®½ã€‚æˆ‘ä»¬ç”¨ä¸‰åˆ†ä¹‹ä¸€çš„è®­ç»ƒæ•°æ®åŒ…æ‹¬æ–¹ä½è§’ï¼ˆ300,320,340,0,20,40ï¼‰å¯¹å·ç§¯åŸºå‡†å’Œèƒ¶å›Šæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶ç”¨ä¸‰åˆ†ä¹‹äºŒæµ‹è¯•æ•°æ®åŒ…å«æ–¹ä½è§’ä»60åˆ°280è¿›è¡Œæµ‹è¯•ã€‚åœ¨å¦ä¸€ä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹3ä¸ªæ›´å°é«˜åº¦è¿›è¡Œè®­ç»ƒå’Œ6ä¸ªè¾ƒå¤§çš„é«˜åº¦è¿›è¡Œæµ‹è¯•ã€‚
å¾ˆéš¾ç¡®å®šèƒ¶å›Šæ¨¡å‹æ˜¯å¦å¯¹æ–°è§†è§’æ€»ä½“ä¸Šæ›´å¥½ï¼Œå› ä¸ºå®ƒåœ¨æ‰€æœ‰è§†è§’ä¸Šï¼Œå®ç°äº†æ›´å¥½çš„æµ‹è¯•å‡†ç¡®æ€§ã€‚ä¸ºäº†æ¶ˆé™¤è¿™ä¸ªæ··æ‚å› ç´ ï¼Œåœ¨ç¬¬ä¸‰æµ‹è¯•é›†ç”¨äºè®­ç»ƒè§†ç‚¹æ—¶ï¼Œèƒ¶å›Šæ¨¡å‹çš„æ€§èƒ½ä¸åŸºå‡†CNNåŒ¹é…ï¼Œæˆ‘ä»¬åœæ­¢è®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ¯”è¾ƒåœ¨ä¸‰åˆ†ä¹‹äºŒæµ‹è¯•é›†ä¸Šçš„åŒ¹é…æ¨¡å‹ä¸æ–°è§†è§’ã€‚è¡¨2çš„ç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œåœ¨ç†Ÿæ‚‰è§†è§’ä¸Šæ€§èƒ½åŒ¹é…çš„èƒ¶å›Šï¼Œåœ¨æ–°è§†è§’ä¸Šï¼Œå¯¹äºæ–°æ–¹ä½è§’å’Œæ–°é«˜ç¨‹ï¼Œå‡å‡å°‘äº†çº¦ä¸º30ï¼…æµ‹è¯•é”™è¯¯ç‡ã€‚

6 ADVERSARIAL ROBUSTNESS
There is growing interest in the vulnerability of neural networks to adversarial examples; inputs
that have been slightly changed by an attacker to trick a neural net classifier into making the wrong
classification. These inputs can be created in a variety of ways, but straightforward strategies such as
FGSM (Goodfellow et al. (2014)) have been shown to drastically decrease accuracy in convolutional
neural networks on image classification tasks. We compare our capsule model and a traditional
convolutional model on their ability to withstand such attacks.
FGSM computes the gradient of the loss w.r.t. each pixel intensity and then changes the pixel
intensity by a fixed amount  in the direction that increases the loss. So the changes only depend on
the sign of the gradient at each pixel. This can be extended to a targeted attack by updating the input
to maximize the classification probability of a particular wrong class. We generated adversarial
attacks using FGSM because it has only one hyper-parameter and it is easy to compare models
that have very different gradient magnitudes. To test the robustness of our model, we generated
adversarial images from the test set using a fully trained model. We then reported the accuracy of
the model on these images.
We found that our model is significantly less vulnerable to both general and targeted FGSM adversarial
attacks; a small  can be used to reduce a convolutional modelâ€™s accuracy much more than an
equivalent  can on the capsule model (Fig. 3). It should also be noted that the capsule modelâ€™s accuracy
after the untargeted attack never drops below chance (20%) whereas the convolutional modelâ€™s
accuracy is reduced to significantly below chance with an  as small as 0.2.
We also tested our model on the slightly more sophisticated adversarial attack of the Basic Iterative
Method (Kurakin et al. (2016)), which is simply the aforementioned attack except it takes multiple
smaller steps when creating the adversarial image. Here too we find that our model is much more
robust to the attack than the traditional convolutional model.

6 å¯¹æŠ—é²æ£’æ€§
äººä»¬å¯¹ç¥ç»ç½‘ç»œåœ¨å¯¹æŠ—æ ·æœ¬æ—¶çš„è„†å¼±æ€§è¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æ”»å‡»è€…ç¨å¾®æ”¹å˜çš„è¾“å…¥å°±ä¼šæ¬ºéª—ç¥ç»ç½‘ç»œåˆ†ç±»å™¨åˆ¶é€ é”™è¯¯åˆ†ç±»ã€‚è¿™äº›è¾“å…¥å¯ä»¥é€šè¿‡å„ç§æ–¹å¼åˆ›å»ºï¼Œä½†ç›´æ¥çš„ç­–ç•¥å¦‚FGSMï¼ˆGoodfellow et al.ï¼ˆ2014ï¼‰ï¼‰å·²ç»æ˜¾ç¤ºå¤§å¤§é™ä½äº†å·ç§¯ç¥ç»ç½‘ç»œæ‰§è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ¯”è¾ƒèƒ¶å›Šæ¨¡å‹å’Œä¼ ç»Ÿå·ç§¯æ¨¡å‹æŠµå¾¡è¿™ç§æ”»å‡»çš„èƒ½åŠ›ã€‚FGSMè®¡ç®—æŸå¤±w.r.tçš„æ¢¯åº¦ï¼Œæ¯ä¸ªåƒç´ å¼ºåº¦ï¼Œç„¶åé€šè¿‡å›ºå®šå€¼$\epsilon$åœ¨æé«˜æŸå¤±çš„æ–¹å‘ä¸Šæ”¹å˜åƒç´ å¼ºåº¦ã€‚è¿™æ ·ï¼Œè¿™äº›å˜åŒ–åªä¾èµ–äºæ¯ä¸ªåƒç´ æ¸å˜çš„ä¿¡å·ã€‚è¿™å¯ä»¥æ‰©å±•åˆ°æˆä¸€ä¸ªé’ˆå¯¹æ€§çš„æ”»å‡»ï¼Œæ–¹æ³•æ˜¯é€šè¿‡æ›´æ–°è¾“å…¥æ¥æœ€å¤§åŒ–ä¸€ä¸ªç‰¹å®šé”™è¯¯ç±»åˆ«çš„åˆ†ç±»æ¦‚ç‡ã€‚æˆ‘ä»¬ä½¿ç”¨FGSMç”Ÿæˆä¸€ä¸ªå¯¹æŠ—æ”»å‡»ï¼Œå› ä¸ºå®ƒåªæœ‰ä¸€ä¸ªè¶…å‚æ•°ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“æ¯”è¾ƒå…·æœ‰éå¸¸ä¸åŒæ¢¯åº¦å¤§å°çš„æ¨¡å‹ã€‚

ä¸ºäº†æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬ç”¨å®Œå…¨è®­ç»ƒçš„æ¨¡å‹ä»æµ‹è¯•é›†ä¸­ç”Ÿæˆå¯¹æŠ—å›¾ç‰‡ã€‚ç„¶åæˆ‘ä»¬æœ‰äº†è¿™äº›å›¾ç‰‡çš„æ¨¡å‹å‡†ç¡®æ€§æŠ¥å‘Šã€‚æˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ¨¡å‹å¯¹äºæ™®é€šå’Œæœ‰é’ˆå¯¹æ€§çš„FGSMæ”»å‡»ï¼Œæ˜æ˜¾åœ°éƒ½ä¸é‚£ä¹ˆè„†å¼±ï¼›ä¸€ä¸ªå°çš„$\epsilon$å¯ä»¥å‡å°‘å·ç§¯æ¨¡å‹çš„ç²¾åº¦è¿œè¿œè¶…è¿‡ä¸€ä¸ªç›¸åŒ$\epsilon$åœ¨èƒ¶å›Šæ¨¡å‹ä¸Šçš„ä½œç”¨ï¼ˆå›¾3ï¼‰ã€‚è¿˜åº”è¯¥æŒ‡å‡ºï¼Œèƒ¶å›Šæ¨¡å‹çš„å‡†ç¡®æ€§åœ¨éé’ˆå¯¹æ€§æ”»å‡»åï¼Œç»ä¸ä¼šé™åˆ°ï¼ˆ20ï¼…ï¼‰ä»¥ä¸‹å‡ ç‡ï¼›è€Œå·ç§¯æ¨¡å‹å‡†ç¡®æ€§ä¼šæ˜æ˜¾åœ°å› ä¸º$\epsilon$è€Œä½åˆ°å…¶å‡ ç‡å°åˆ°0.2ã€‚æˆ‘ä»¬è¿˜æµ‹è¯•äº†ç¨å¾®å¤æ‚çš„å¯¹æŠ—æ”»å‡»ï¼Œå³åŸºæœ¬è¿­ä»£æ–¹æ³•ï¼ˆKurakin et al.ï¼ˆ2016ï¼‰ï¼‰ï¼Œå…¶å°±æ˜¯ä¸Šè¿°æ”»å‡»ï¼Œåªæ˜¯åˆ›å»ºæ”»å‡»å›¾ç‰‡æ—¶é‡‡å–å¤šä¸ªæ›´å°æ­¥éª¤ã€‚è¿™ä¹Ÿè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹æ¯”ä¼ ç»Ÿçš„å·ç§¯æ¨¡å‹å…·æœ‰å¼ºå¾—å¤šçš„æŠ—å‡»åŠ›ã€‚

![å›¾3](https://github.com/humor250/matrixcapsules/blob/master/pic3_matrixcapsules.png)

å›¾3ï¼šå¯¹æŠ—æ”»å‡»ï¼ˆå·¦ï¼‰åçš„$\epsilon$å‡†ç¡®æ€§å’Œç›®æ ‡æ”»å‡»ï¼ˆå³ï¼‰åçš„æˆåŠŸç‡ã€‚ç›®æ ‡æ”»å‡»ç»“æœï¼Œæ˜¯å¯¹5ä¸ªå¯èƒ½ç§ç±»çš„æ¯ä¸€ä¸ªçš„æ”»å‡»åï¼Œé€šè¿‡å¹³å‡æˆåŠŸç‡è¿›è¡Œè¯„ä¼°ã€‚

å·²ç»è¡¨æ˜ï¼Œæ¨¡å‹ä¸­å¯¹æŠ—æ”»å‡»çš„ä¸€äº›é²æ£’æ€§å¯èƒ½ç”±äºåœ¨æ¢¯åº¦Brendelï¼†Bethgeï¼ˆ2017ï¼‰è®¡ç®—ä¸­ç®€å•æ•°å­—çš„ä¸ç¨³å®šæ€§ã€‚ä¸ºäº†ç¡®ä¿è¿™ä¸æ˜¯æˆ‘ä»¬æ¨¡å‹ç¨³å¥æ€§çš„å”¯ä¸€åŸå› ï¼Œé’ˆå¯¹èƒ¶å›Šæ¨¡å‹ä¸­çš„å›¾åƒï¼Œæˆ‘ä»¬è®¡ç®—äº†æ¢¯åº¦ä¸­é›¶å€¼çš„ç™¾åˆ†æ¯”ï¼Œå¹¶ä¸”å‘ç°å…¶å°äºCNNã€‚æ­¤å¤–ï¼Œèƒ¶å›Šæ¢¯åº¦è™½ç„¶å°äºCNNï¼Œä½†åªå°äº†2ä¸ªæ•°é‡çº§ï¼Œè€Œä¸æ˜¯åœ¨Brendelï¼†Bethgeï¼ˆ2017ï¼‰çš„å·¥ä½œæ‰€è§çš„16ä¸ªæ•°é‡çº§ã€‚
æœ€åï¼Œæˆ‘ä»¬æµ‹è¯•æˆ‘ä»¬æ¨¡å‹å¯¹é»‘åŒ£å­æ”»å‡»çš„é²æ£’æ€§ã€‚é€šè¿‡ç”¨ä¸€ä¸ªCNNç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œå¹¶åœ¨èƒ¶å›Šæ¨¡å‹å’Œä¸åŒçš„CNNä¸Šæµ‹è¯•å®ƒä»¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œèƒ¶å›Šæ¨¡å‹åœ¨è¿™é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°å¹¶ä¸æ¯”CNNå¥½å¾—å¤šã€‚

7 RELATED WORKç›¸å…³å·¥ä½œ
Among the multiple recent attempts at improving the ability of neural networks to deal with viewpoint
variations, there are two main streams. One stream attempts to achieve viewpoint invariance
and the other aims for viewpoint equivariance. The work presented by Jaderberg et al. (2015)), Spatial
Transformer Networks, seeks viewpoint invariance by changing the sampling of CNNs according
to a selection of affine transformations. De Brabandere et al. (2016) extends spatial transformer
networks where the filters are adapted during inference depending on the input. 

They generate different
filters for each locality in the feature map rather than applying the same transformation to all
filters. 
åœ¨æœ€è¿‘å¤šæ¬¡å°è¯•æé«˜ç¥ç»ç½‘ç»œå¤„ç†è§†ç‚¹å˜åŒ–èƒ½åŠ›æ–¹é¢ï¼Œæœ‰ä¸¤ä¸ªä¸»æµã€‚ä¸€ä¸ªå°è¯•å®ç°è§†ç‚¹ä¸å˜æ€§å¦ä¸€ä¸ªç›®æ ‡æ˜¯è§†ç‚¹åŒå˜æ€§ã€‚Jaderbergç­‰äººæå‡ºçš„å·¥ä½œã€‚ï¼ˆ2015ï¼‰ï¼‰ï¼Œç©ºé—´è½¬æ¢ç½‘ç»œï¼ŒæŒ‰ç…§é€‰æ‹©ä¸€ä¸ªä»¿å°„å˜æ¢ï¼Œé€šè¿‡æ”¹å˜CNNçš„é‡‡æ ·æ¥å¯»æ±‚è§†ç‚¹ä¸å˜æ€§ã€‚ De Brabandereç­‰äººï¼ˆ2016å¹´ï¼‰æ‰©å±•äº†ç©ºé—´è½¬æ¢å™¨ç½‘ç»œï¼Œåœ¨è¾“å…¥æ¨ç†æ—¶é‡‡ç”¨è¿‡æ»¤å™¨ã€‚ä»–ä»¬ä¸ºç‰¹å¾æ˜ å°„ä¸­çš„æ¯ä¸ªä½ç½®ç”Ÿæˆä¸åŒçš„è¿‡æ»¤å™¨ï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰è¿‡æ»¤å™¨åº”ç”¨åŒä¸€è½¬æ¢ã€‚

Their approach is a step toward input covariance detection from traditional pattern matching
frameworks like standard CNNs (LeCun et al. (1990)). Dai et al. (2017) improves upon spatial
transformer networks by generalizing the sampling method of filters. 

Our work differs substantially in that a unit is not activated based on the matching score with a filter (either fixed or dynamically changing during inference). 
In our case, a capsule is activated only if the transformed poses coming from the layer below match each other. 

This is a more effective way to capture covariance and leads to models with many fewer parameters that generalize better.

The success of CNNs has motivated many researchers to extend the translational equivariance built in to CNNs to include rotational equivariance (Cohen & Welling (2016), Dieleman et al. (2016), Oyallon & Mallat (2015)). 

The recent approach in Harmonic Networks (Worrall et al. (2017))
achieves rotation equivariant feature maps by using circular harmonic filters and returning both the
maximal response and orientation using complex numbers. 

ä»–ä»¬çš„æ–¹æ³•æ˜¯ä»ä¼ ç»Ÿæ¨¡å¼åŒ¹é…æ¡†æ¶ï¼Œå¦‚æ ‡å‡†CNNï¼ˆLeCun et alã€‚ï¼ˆ1990ï¼‰ï¼‰. Dai et al.ï¼ˆ2017å¹´ï¼‰å‘è¾“å…¥åå˜æ€§æ£€æµ‹è¿ˆå‡ºçš„ä¸€æ­¥ï¼Œè¿™æ ·é€šè¿‡æ¨å¹¿æ»¤æ³¢å™¨çš„æŠ½æ ·æ–¹æ³•æå‡ç©ºé—´è½¬æ¢ç½‘ç»œã€‚
æˆ‘ä»¬çš„å·¥ä½œå¾ˆå¤§ä¸åŒäºé‚£ç§ï¼šæ ¹æ®è¿‡æ»¤å™¨ï¼ˆå›ºå®šæˆ–åœ¨æ¨æ–­æœŸé—´åŠ¨æ€æ”¹å˜ï¼‰çš„åŒ¹é…åˆ†æ•°ï¼Œç¡®å®šå•å…ƒæ˜¯å¦æ¿€æ´»ã€‚
åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œåªæœ‰åœ¨è½¬æ¢å§¿åŠ¿æ¥è‡ªäºå½¼æ­¤åŒ¹é…çš„ä¸‹å±‚æ—¶ï¼Œä¸€ä¸ªèƒ¶å›Šæ‰ä¼šæ¿€æ´»ã€‚è¿™æ˜¯ä¸€ä¸ªæ•è·åå˜æ€§å’Œå¼•å¯¼å‡ºæ›´å¥½æ›´å°‘å‚æ•°æ¨¡å‹çš„æ›´æœ‰æ•ˆæ–¹æ³•ã€‚

CNNçš„æˆåŠŸæ¿€å‘äº†è®¸å¤šç ”ç©¶äººå‘˜æ‰©å±•é’ˆå¯¹CNNæ‰€å†…å»ºçš„åŒå˜æ€§ï¼Œä»¥åŒ…å®¹æ—‹è½¬åŒå˜æ€§ï¼ˆCohenï¼†Wellingï¼ˆ2016ï¼‰ï¼ŒDielemanç­‰ï¼ˆ2016ï¼‰ï¼Œ
Oyallonï¼†Mallatï¼ˆ2015ï¼‰ï¼‰ã€‚ 

Harmonic Networksæœ€è¿‘è¿›å±•æ˜¯ï¼ˆWorrall et alã€‚ï¼ˆ2017ï¼‰ï¼‰é€šè¿‡ä½¿ç”¨åœ†è°æ³¢æ»¤æ³¢å™¨å’Œç”¨å¤æ‚æ•°å­—è¿”å›æœ€å¤§å“åº”å’Œæ–¹å‘å®ç°æ—‹è½¬åŒå˜ç‰¹å¾æ˜ å°„ï¼ˆFeature Mapï¼‰ã€‚

This shares the basic representational
idea of capsules: By assuming that there is only one instance of the entity at a location, we can
use several different numbers to represent its properties. They use a fixed number of streams of
rotation orders. By enforcing the equality of the sum of rotation orders along any path, they achieve
patch-wise rotation equivariance. This approach is more parameter-efficient than data augmentation
approaches, duplicating feature maps, or duplicating filters (Fasel & Gatica-Perez (2006), Laptev
et al. (2016)). Our approach encodes general viewpoint equivariance rather than only affine 2D
rotations. Symmetry networks (Gens & Domingos (2014)) use iterative Lucas-Kanade optimization
to find poses that are supported by the most low-level features. Their key weakness is that the
iterative algorithm always starts at the same pose, rather than the mean of the bottom-up votes.

è¿™é‡Œåˆ†äº«åŸºæœ¬çš„å…·æœ‰ä»£è¡¨æ€§çš„èƒ¶å›Šç½‘ç»œçš„æ€æƒ³ï¼šå‡è®¾ä¸€ä¸ªä½ç½®åªæœ‰ä¸€ä¸ªå®ä½“å®ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‡ ä¸ªä¸åŒçš„æ•°å­—æ¥è¡¨ç¤ºå®ƒçš„å±æ€§ã€‚ä»–ä»¬ä½¿ç”¨å›ºå®šæ•°é‡æµçš„æ—‹è½¬æ¬¡åºã€‚é€šè¿‡æ²¿ç€ä»»ä½•è·¯å¾„æ‰§è¡Œæ—‹è½¬æ¬¡åºæ€»å’Œç­‰å¼ï¼Œä»–ä»¬å®ç°è¡¥ä¸å¼æ—‹è½¬åŒå˜æ€§ã€‚è¿™ç§æ–¹æ³•æ¯”æ•°æ®å¢å¼ºæ›´å…·å‚æ•°æ•ˆç‡æ–¹æ³•ï¼Œå¤åˆ¶ç‰¹å¾åœ°å›¾æˆ–å¤åˆ¶è¿‡æ»¤å™¨ï¼ˆFaselï¼†Gatica-Perezï¼ˆ2006ï¼‰ï¼ŒLaptevç­‰äººï¼ˆ2016ï¼‰ï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡ç¼–ç ä¸€èˆ¬è§†ç‚¹åŒå˜æ€§è€Œä¸æ˜¯ä»…ä»¿å°„äºŒç»´æ—‹è½¬ã€‚å¯¹ç§°ç½‘ç»œï¼ˆGensï¼†Domingosï¼ˆ2014ï¼‰ï¼‰ä½¿ç”¨è¿­ä»£Lucas-Kanadeä¼˜åŒ–æ‰¾åˆ°æœ€ä½çº§ç‰¹å¾æ”¯æŒçš„å§¿æ€ã€‚ä»–ä»¬çš„å…³é”®å¼±ç‚¹æ˜¯ï¼Œè¿­ä»£ç®—æ³•æ€»æ˜¯å§‹äºç›¸åŒçš„å§¿åŠ¿ï¼Œè€Œä¸æ˜¯è‡ªä¸‹è€Œä¸Šé€‰ç¥¨çš„å‡å€¼ã€‚

Lenc & Vedaldi (2016) proposes a feature detection mechanism (DetNet) that is equivariant to affine
transformations. DetNet is designed to detect the same points in the image under different viewpoint
variations. This effort is orthogonal to our work but DetNet might be a good way to implement the
de-rendering first-stage that activates the layer of primary capsules.
Our routing algorithm can be seen as an attention mechanism. In this view, it is related to the work of
Gregor et al. (2015), where they improved the decoder performance in a generative model by using
Gaussian kernels to attend to different parts of the feature map generated by the encoder. Vaswani
et al. (2017) uses a softmax attention mechanism to match parts of the query sequence to parts of
the input sequence for the translation task and when generating an encoding for the query. They
show improvement upon previous translation efforts using recurrent architectures. Our algorithm
has attention in the opposite direction. The competition is not between the lower-level capsules that
a higher-level capsule might attend to. It is between the higher-level capsules that a lower-level
capsule might send its vote to.

Lencå’ŒVedaldiï¼ˆ2016ï¼‰æå‡ºäº†ä¸€ä¸ªä¸ä»¿å°„åŒå˜çš„ç‰¹å¾æ£€æµ‹æœºåˆ¶ï¼ˆDetNetï¼‰ã€‚DetNetæ—¨åœ¨æ£€æµ‹ä¸åŒè§†ç‚¹å˜åŒ–ä¸‹çš„å›¾åƒçš„ç›¸åŒç‚¹ã€‚è¿™é¡¹å·¥ä½œä¸æˆ‘ä»¬çš„å·¥ä½œæ˜¯æ­£äº¤çš„ï¼Œä½†DetNetå¯èƒ½æ˜¯å®ç°è§£é™¤æ¸²æŸ“çš„ç¬¬ä¸€é˜¶æ®µï¼Œå³æ¿€æ´»ä¸»èƒ¶å›Šå±‚è¿™é¡¹å·¥ä½œçš„ä¸€ä¸ªå¥½æ–¹æ³•ã€‚æˆ‘ä»¬çš„è·¯ç”±ç®—æ³•å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§å…³æ³¨æœºåˆ¶ã€‚åœ¨è¿™ä¸ªè§‚ç‚¹ä¸Šï¼Œå®ƒä¸Gregor et al. (2015)ç­‰äººçš„å·¥ä½œç›¸å…³ï¼Œé€šè¿‡ä½¿ç”¨é«˜æ–¯å†…æ ¸å‚ä¸ç”±ç¼–ç å™¨ç”Ÿæˆçš„ç‰¹å¾æ˜ å°„çš„ä¸åŒéƒ¨åˆ†ï¼Œä»–ä»¬æ”¹è¿›äº†ä¸€ä¸ªç”Ÿæˆæ¨¡å‹çš„è§£ç å™¨æ€§èƒ½ã€‚Vaswani
et al. (2017)ä½¿ç”¨softmaxå…³æ³¨æœºåˆ¶ï¼Œé’ˆå¯¹ç¿»è¯‘ä»»åŠ¡å’Œä¸ºæŸ¥è¯¢ç”Ÿæˆç¼–ç æ—¶ï¼Œå°†æŸ¥è¯¢åºåˆ—éƒ¨åˆ†ä¸è¾“å…¥åºåˆ—éƒ¨åˆ†è¿›è¡ŒåŒ¹é…ã€‚ä»–ä»¬ä½¿ç”¨å¾ªç¯æ¶æ„æ˜¾ç¤ºå‡ºå¯¹ä»¥å‰ç¿»è¯‘å·¥ä½œçš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç®—æ³•å…³æ³¨åœ¨ç›¸åçš„æ–¹å‘ã€‚ç«äº‰ä¸åœ¨ä¸€ä¸ªé«˜çº§èƒ¶å›Šå¯èƒ½å‚åŠ çš„ä½çº§èƒ¶å›Šä¹‹é—´ï¼Œè€Œåœ¨ä¸€ä¸ªä½çº§èƒ¶å›Šå¯èƒ½ä¼šå°†å…¶é€‰ç¥¨å‘ç»™çš„é«˜çº§èƒ¶å›Šä¹‹é—´ã€‚

7.1 PREVIOUS WORK ON CAPSULES
Hinton et al. (2011) used a transformation matrix in a transforming autoencoder that learned to
transform a stereo pair of images into a stereo pair from a slightly different viewpoint. However,
that system requires the transformation matrix to be supplied externally. More recently, routing-byagreement
was shown to be effective for segmenting highly overlapping digits (Sabour et al. (2017)),
but that system has several deficiencies that we have overcome in this paper:
1. It uses the length of the pose vector to represent the probability that the entity represented by
a capsule is present. To keep the length less than 1, requires an unprincipled non-linearity
and this prevents the existence of any sensible objective function that is minimized by the
iterative routing procedure.
2. It uses the cosine of the angle between two pose vectors to measure their agreement. Unlike
the negative log variance of a Gaussian cluster, the cosine saturates at 1, which makes it
insensitive to the difference between a quite good agreement and a very good agreement.
3. It uses a vector of length n rather than a matrix with n elements to represent a pose, so its
transformation matrices have n
2 parameters rather than just n.

7.1 èƒ¶å›Šç½‘ç»œçš„å‰æœŸå·¥ä½œ

Hintonç­‰äººï¼ˆ2011ï¼‰åœ¨ä¸€ä¸ªå˜æ¢è‡ªç¼–ç å™¨ä¸­ä½¿ç”¨äº†ä¸€ä¸ªå˜æ¢çŸ©é˜µï¼Œè‡ªè§£ç å™¨çŸ¥é“å¦‚ä½•å°†ä¸€å¯¹ç«‹ä½“å›¾åƒè½¬æ¢ä¸ºç•¥å¾®ä¸åŒè§†ç‚¹çš„ç«‹ä½“å¯¹ã€‚ç„¶è€Œï¼Œè¯¥ç³»ç»Ÿéœ€è¦ä»å¤–éƒ¨æä¾›å˜æ¢çŸ©é˜µã€‚æœ€è¿‘ï¼Œåè®®è·¯ç”±è¢«è¯æ˜å¯¹åˆ†å‰²é«˜åº¦é‡å çš„æ•°å­—æ˜¯æœ‰æ•ˆçš„ï¼ˆSabourç­‰ï¼ˆ2017ï¼‰ï¼‰ï¼Œä½†æ˜¯è¿™ä¸ªç³»ç»Ÿæœ‰å‡ ä¸ªç¼ºé™·ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼š

1. å®ƒä½¿ç”¨å§¿æ€å‘é‡çš„é•¿åº¦ï¼Œæ¥è¡¨ç¤ºç”±ä¸€ä¸ªèƒ¶å›Šè¡¨ç¤ºçš„å®ä½“å­˜åœ¨çš„æ¦‚ç‡ã€‚è¦ä¿æŒé•¿åº¦å°äº1ï¼Œéœ€è¦ä¸€ä¸ªæ— åŸåˆ™çš„éçº¿æ€§ï¼Œå¹¶ä¸”è¿™å¯ä»¥é˜²æ­¢ä»»ä½•ç”±è¿­ä»£è·¯ç”±ç¨‹åºæœ€å°åŒ–çš„æ˜æ™ºçš„ç›®æ ‡å‡½æ•°çš„å­˜åœ¨ã€‚
2. å®ƒä½¿ç”¨ä¸¤ä¸ªå§¿æ€çŸ¢é‡å¤¹è§’çš„ä½™å¼¦æ¥è¡¡é‡å®ƒä»¬çš„ä¸€è‡´æ€§ã€‚ä¸åƒé«˜æ–¯ç°‡çš„è´Ÿå¯¹æ•°æ–¹å·®ï¼Œä½™å¼¦åœ¨1å¤„é¥±å’Œï¼Œè¿™ä¼šä½¿å®ƒå¯¹ç›¸å½“å¥½çš„åè®®å’Œéå¸¸å¥½çš„åè®®çš„åŒºåˆ«ä¸æ•æ„Ÿã€‚
3. å®ƒç”¨ä¸€ä¸ªé•¿åº¦ä¸ºnçš„çŸ¢é‡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæœ‰nä¸ªå…ƒç´ çš„çŸ©é˜µæ¥è¡¨ç¤ºä¸€ä¸ªå§¿åŠ¿ï¼Œæ‰€ä»¥å®ƒçš„å˜æ¢çŸ©é˜µæœ‰$n^2$ä¸ªå‚æ•°è€Œä¸ä»…ä»…æ˜¯nã€‚

8 CONCLUSION
Building on the work of Sabour et al. (2017), we have proposed a new type of capsule system in
which each capsule has a logistic unit to represent the presence of an entity and a 4x4 pose matrix
to represent the pose of that entity. We also introduced a new iterative routing procedure between
capsule layers, based on the EM algorithm, which allows the output of each lower-level capsule
to be routed to a capsule in the layer above in such a way that active capsules receive a cluster of
similar pose votes. This new system achieves significantly better accuracy on the smallNORB data
set than the state-of-the-art CNN, reducing the number of errors by 45%. We have also shown it to
be significantly more robust to white box adversarial attacks than a baseline CNN.
SmallNORB is an ideal data-set for developing new shape-recognition models precisely because it
lacks many of the additional features of images in the wild. Now that our capsules model works
well on NORB, we plan to implement an efficient version to test much larger models on much larger
data-sets such as ImageNet.
ACKNOWLEDGMENTS Thanks to Robert Gens, Eric Langlois, Taco Cohen and anonymous
commentators for helpful discussions and to everyone who made TensorFlow.

8 ç»“è®º
ä»¥Sabourç­‰äººï¼ˆ2017å¹´ï¼‰çš„å·¥ä½œä¸ºåŸºç¡€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹èƒ¶å›Šç³»ç»Ÿï¼Œå…¶ä¸­æ¯ä¸ªèƒ¶å›Šæœ‰ä¸€ä¸ªé€»è¾‘å•å…ƒè¡¨ç¤ºå®ä½“çš„å­˜åœ¨å’Œ4Ã—4å§¿æ€çŸ©é˜µè¡¨ç¤ºè¯¥å®ä½“çš„å§¿æ€ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºäºEMç®—æ³•çš„åœ¨èƒ¶å›Šå±‚ä¹‹é—´çš„è¿­ä»£è·¯ç”±ç¨‹åºï¼Œå…¶å…è®¸æ¯ä¸ªè¾ƒä½çº§èƒ¶å›Šçš„è¾“å‡ºè¢«è·¯ç”±åˆ°ä¸Šå±‚çš„ä¸€ä¸ªèƒ¶å›Šï¼Œä½¿å¾—æ´»æ€§èƒ¶å›Šæ¥æ”¶ä¸€ç°‡è¿‘ä¼¼çš„å§¿åŠ¿é€‰ç¥¨ã€‚è¿™ä¸ªæ–°ç³»ç»Ÿåœ¨smallNORBæ•°æ®ä¸Šæ¯”æœ€ä¼˜çš„CNNå®ç°äº†æ›´é«˜çš„ç²¾åº¦ï¼Œå‡å°‘äº†45ï¼…çš„é”™è¯¯é‡ã€‚æˆ‘ä»¬ä¹Ÿé˜ç¤ºäº†å®ƒå¯¹äºç™½ç›’å¯¹æŠ—æ€§æ”»å‡»æ¯”åŸºå‡†CNNå…·æœ‰æ˜¾è‘—çš„æŠµæŠ—åŠ›ã€‚SmallNORBæ˜¯å¼€å‘æ–°å‹å½¢çŠ¶è¯†åˆ«æ¨¡å‹çš„ç†æƒ³æ•°æ®é›†ï¼Œå› ä¸ºå®ƒæ°æ°ç¼ºä¹è®¸å¤šé¢å¤–å¹²æ‰°æ€§å›¾åƒç‰¹å¾ã€‚ç°åœ¨æˆ‘ä»¬çš„èƒ¶å›Šæ¨¡å‹åœ¨NORBä¸Šå·¥ä½œå¾—å¾ˆå¥½ï¼Œæˆ‘ä»¬è®¡åˆ’å®æ–½ä¸€ä¸ªé«˜æ•ˆç‰ˆæœ¬åœ¨å¤§å¾—å¤šçš„æ•°æ®é›†å¦‚ImageNetä¸Šæµ‹è¯•æ›´å¤§çš„æ¨¡å‹ã€‚

å£°æ˜ï¼šæ„Ÿè°¢Robert Gensï¼ŒEric Langloisï¼ŒTaco Cohenå’Œæœ‰åŠ©äºè®¨è®ºçš„åŒ¿åè¯„è®ºå‘˜ä»¥åŠæ¯ä¸ªTensorFlowçš„åˆ›å»ºè€…ã€‚
